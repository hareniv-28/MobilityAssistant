# ğŸš¶â€â™€ï¸ Mobility Assistant

**Mobility Assistant** is an Android app that uses on-device AI to detect nearby **people, bicycles, and vehicles**, helping visually impaired users navigate safely.  
It gives **audio and haptic feedback**, runs **offline**, and stays active **even with the screen off**.

---

## ğŸŒŸ Key Features
- ğŸ‘ï¸ Real-time object detection (on-device TensorFlow Lite)
- ğŸ”Š Bilingual voice alerts (English + Indian language)
- ğŸ“³ Haptic feedback for distance and direction
- ğŸŒ™ Context awareness (adapts to light, motion, and noise)
- ğŸ“´ Screen-off detection mode
- âš™ï¸ Runs fully offline â€” no internet needed

---

## ğŸ§  Tech Stack
| Area | Technology |
|-------|-------------|
| Language | Kotlin |
| ML Framework | TensorFlow Lite |
| Camera | CameraX |
| UI | ViewBinding |
| Feedback | Text-to-Speech + Haptics |

---

## ğŸ—ï¸ Current Progress
âœ… App structure, CameraX integration, and permissions done  
ğŸš§ Model integration in progress (TensorFlow Lite setup)  
ğŸ¯ Next: object detection overlay + audio/haptic responses

---

## ğŸš€ How to Run
1. Clone the repo:
   ```bash
   git clone https://github.com/hareniv-28/MobilityAssistant.git

Open in Android Studio.

2. Sync Gradle files.

3. Run on a real Android device (Android 9.0 or higher).

4. Grant camera permissions when prompted.

ğŸ§© Future Roadmap

-> Add pothole and road hazard detection

-> Integrate GPS-based guidance

-> Optimize inference for low-end devices

-> Add gesture-based controls

ğŸ‘©â€ğŸ’» Author

Hareni V
Computer Science Student | Android & AI Developer
